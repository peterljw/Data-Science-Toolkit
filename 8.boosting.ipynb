{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to bagging, boosting is a general approach that can be applied to predictive models for regression or classfication.\n",
    "\n",
    "Recall that bagging trains mutiple models on boostrapped data in parallel, that is, each model is built on a boostrap dataset independent of the other models. Boosting trains the models sequentially, and each model is built using information from previusly built models. In addition, boosting tends to use weaker learners as its base learner while bagging tends to use more complex learners.\n",
    "\n",
    "Boosting is also commly combined with trees, often stumps (single split trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "Main concepts:\n",
    "- Weak base learner, often a stump\n",
    "- The weight or amount of say of each tree is based on their performance\n",
    "- Trees are built sequentially\n",
    "\n",
    "The weight of a tree$_i$ is determined by $\\frac{1}{2}log(\\frac{1-e_i}{e_i})$, where $e_i$ is the total error.\n",
    "\n",
    "The first tree is built on data with uniformly distributed sample weight. Each each sequential tree is built with adjusted sample weight based on the performance of the previous tree.\n",
    "\n",
    "Sample weight of each data point is adjusted as follows,\n",
    "\n",
    "- If misclassified, weight$^{new}$ = weight$^{old}*e^{amount of say}$\n",
    "- If correctly classified, weight$^{new}$ = weight$^{old}*e^{-amount of say}$\n",
    "\n",
    "Normalize the new sample weights to add up to 1 after adjustments.\n",
    "\n",
    "In other words, increase the weight of misclassified data while reducing the correctly classified data so that the sequential models can focous on targeting the misclassified data.\n",
    "\n",
    "Then train the new train either by (i) using weighted gini gain or (ii) boostrapping a new training dataset based on the new sample weights.\n",
    "\n",
    "Stop when reaching a pre-specified number of trees or converging to perfect fit.\n",
    "\n",
    "\n",
    "### Gradient Boost\n",
    "\n",
    "Main concepts:\n",
    "- Base learners are decision trees, larger than stumps but still restricted (usually 8-32 leafs)\n",
    "- Additive model, the final result is a linear combinations of the trees with learning rates as coefficients\n",
    "- Trees are built sequentially\n",
    "\n",
    "Initial prediction:\n",
    "- mean for regression\n",
    "- Sigmoid(log(odds)) for classification\n",
    "\n",
    "Sequential trees fit to the pseudo residuals from the previous predictions. Some transformations are needed for classifications to use the predictions to update log(odds).\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "Extreme gradient boosting\n",
    "\n",
    "Main concepts:\n",
    "- Adds regularization terms to loss functions\n",
    "- More computationally efficient than GB\n",
    "- Supports parallel processing during the construction of each tree\n",
    "- Build trees differently, similarity gain instead of gini/cross-entropy\n",
    "- Uses the 2nd degree Taylor expansion while normal GB only uses the 1st degree (faster convergence and compatibility with other loss functions from a derivation standpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
