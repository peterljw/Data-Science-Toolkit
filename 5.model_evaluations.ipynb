{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "We can split the dataset into k folds, training the models on k-1 groups and testing on the remaining group of data. After repeating such procedure k times, we would have k estimates of the test error (e..g take the mean, median, etc). Using the estimated test errors, we prefer to choose a model that commits the least test errors.\n",
    "\n",
    "Cross-validation can be used to estimate the performance of a model on unseen dataset, which can then help with\n",
    "- Detecting overfitting\n",
    "- Tuning hyperparameters\n",
    "- Model selection\n",
    "\n",
    "The error estimate from cross-validation could potentially be an over-estimation since the hold-out set is not used to train the model, reducing the amount of training data. Common choices for k are 5 to 10 depending on the datasize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Evaluation\n",
    "\n",
    "Most common:\n",
    "- Root Mean Square Error(RMSE): RSS divided by n then root\n",
    "- Mean Absolute Error(MAE): Penalizes big prediction error less in comparison to RMSE\n",
    "\n",
    "Old school:\n",
    "- Residual Standard Error(RSE): An expression of RSS accounting for the degree of freedom, penalizing the number of features\n",
    "- $R^2$: The fraction of variance explained by the model, always favors more complex models\n",
    "- Adjusted $R^2$: Unlike the R2 statistic, the adjusted R2 statistic pays a price for the inclusion of unnecessary variables in the model\n",
    "- $AIC$, $BIC$, $C_p$: Relative measures to compare models, penalize model complexity and account for the model's generalization\n",
    "\n",
    "Others:\n",
    "- Compare to a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Evaluation\n",
    "\n",
    "- Accuracy: \n",
    "    - (TP+TN)/(TP+FP+FN+TN)\n",
    "    - The proportion of true results among the total number of cases examined\n",
    "    - Valid for classification problems which are well balanced and not skewed or no class imbalance\n",
    "- Positive Predictive Value(PPV) (Precision): \n",
    "    - (TP)/(TP+FP)\n",
    "    - The proportion of correct predictions among positive predictions\n",
    "    - Valid when we want to be very sure of our positive prediction\n",
    "- Negative Predictive Value(NPV):\n",
    "    - TN/(TN+FN)\n",
    "    - The proportion of correct predictions among negative predictions\n",
    "    - Valid when a false negative prediction is costly to the problem\n",
    "- True Positive Rate (TPR) (Recall, Sensitivity):\n",
    "    - (TP)/(TP+FN)\n",
    "    - The proportion of positives that are correctly predicted\n",
    "    - Valid when we want to capture as many positives as possible\n",
    "- True Negative Rate (TNR) (Specificity, Selectivity):\n",
    "    - (TN)/(TN+FP)\n",
    "    - The proportion of negatives that are correctly predicted\n",
    "    - Valid when we want to capture as many negatives as possible\n",
    "- F1 Score:\n",
    "    - 2*((Precision*Recall) / (Precision+Recall))\n",
    "    - A number between 0 and 1 and is the harmonic mean of precision and recall\n",
    "    - Valid when we want to have a model with both good precision and recall (which has a trade-off relationship)\n",
    "- AUC or ROC Curve\n",
    "    - a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "        - True Positive Rate\n",
    "        - False Positive Rate\n",
    "    - AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n",
    "    - AUC is great when assessing the overall performance of the model because\n",
    "        - AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
    "        - AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
