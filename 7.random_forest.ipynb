{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging (aka Boostrap aggregation) is a general-purpose for reducing the variance of a model. It is commonly used in the context of decision trees. \n",
    "\n",
    "Motivation: By CLT, a set of n independent observations, $Z_1,...,Z_n$, each with variance $\\sigma^2$, the variance of $\\bar{Z}$ is $\\frac{\\sigma^2}{n}$. So averaging a set of observations reduces variance. However, this is not practical because we don't have access to n training sets.\n",
    "\n",
    "Bagging tries to acheive this, but through boostraping. We can take the average of $B$ predictions for regression, and the majority vote of $B$ predictions for classification. Estimating the error is also convenient with out-of-bag data (usually around $\\frac{1}{3}$ due to boostraping)\n",
    "\n",
    "We generate $B$ different bootstrapped training datasets, and train $B$ models. The prediction at the data point x will be given by\n",
    "\n",
    "$\\hat{f}_{bagging}(x) = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^{*b}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests is an extension from bagging that tries to decorrelate the trees. On top of bagging, a random selection of $m$ features are chosen from the fullset of p features each time a split is made in the process of building a tree. Common choices are $m=\\sqrt{p}$ or $m=\\frac{p}{3}$.\n",
    "\n",
    "**Common Hyperparameters**\n",
    "- Number of trees\n",
    "- Maximum depth of each tree\n",
    "- Choice of m\n",
    "\n",
    "**Variable Importancce**\n",
    "\n",
    "It's hard to interput a Random Forests in the way we do to a decision tree. Its prediction consists of an aggregation of mutiple trees and each tree will have used a subset of the features to as splitting variables (likely overlappig but not identical), so we can't really study the tree structure anymore for interpretation anymore.\n",
    "\n",
    "However, the nature of Random Forests allows us to assess **feature importance**. In each tree, feature importance is calculated as the decrease in node impurity/entropy measures weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.\n",
    "\n",
    "There are mutiple ways to calculate feature importance depending on the choice of  \n",
    "\n",
    "*Scikit-learn Feature Importance Derivation*\n",
    "\n",
    "https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\n",
    "\n",
    "For each decision tree, *Scikit-learn* calculates a nodes importance using Gini Importance.\n",
    "\n",
    "The importance for each feature on a decision tree is then calculated as follows,\n",
    "\n",
    "$f_i = \\frac{\\sum_{j}n_{ij}}{\\sum_{k}n_{ik}}$, where \n",
    "- $j$: node j which splits on feature i\n",
    "- $k$: all non-terminal nodes in the tree\n",
    "- $n_{ij}$ is the node importance, which is calculated by $n_{ij} = w_jg_j - w^{left}_{j}g^{left}_j - w^{right}_{j}g^{right}_j$, where\n",
    "    - $w_j$ = weighted number of samples reaching node j\n",
    "    - $g_j$ = the gini impurity value at node j\n",
    "    - left: left child node of the split from node j\n",
    "    - right: right child node of the split from node j\n",
    "    \n",
    "Extending this to Random Forest, the importance of a feature is\n",
    "\n",
    "$RFf_i = \\frac{\\sum_tnormf_{it}}{T}$, where\n",
    "- t: all trees\n",
    "- T: the total number of trees\n",
    "- $normf_{it}$: normalized feature importance of feature i in tree t\n",
    "    - $normf_{i} = \\frac{f_i}{\\sum_pf_{ip}}$, where p: all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
