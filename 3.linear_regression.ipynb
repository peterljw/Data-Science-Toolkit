{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Intro\n",
    "\n",
    "**Model**\n",
    "\n",
    "We assume a model $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon$, where y is a continuous variable, and fit \n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + ... + \\hat{\\beta_p}x_p$ \n",
    "\n",
    "to the data by estimating $\\hat{\\beta_i}$'s.\n",
    "\n",
    "**Model Assumptions**:\n",
    "\n",
    "- **Linearity between X’s and Y**: The expected value of the residuals is 0 across all predicted values Y values for all independent variables, holding the others fixed. It can be validated with a residual plot with Y values on the horizontal axis. Observe the behavior of the residuals to ensure they are consistently symmetric with respect to the zero reference line.\n",
    "\n",
    "- **Homoscedasticity (constant variance) of errors**: It can be validated with a residual plot with the predicted Y values on the horizontal axis. Observe the behavior of the residuals to ensure they display a consistent spread with respect to the zero reference line.\n",
    "\n",
    "- **Normality of errors**: The errors need to be normally distributed. It can be checked with assessing the normality of the residuals through hypothesis testing or QQ plot w/ quantiles of a normal distribution.\n",
    "\n",
    "- **Multicollinearity** is not present. It can be validated by the correlation matrix, tolerance values, as well as the Variance Inflation Factor (VIF).\n",
    "\n",
    "- **Statistical independence** of the errors and observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Ordinary Least Squares (OLS)\n",
    "\n",
    "We can obtain the estimators for $\\hat{\\beta_i}$'s by minimizing the *RSS* (Residual Sum of Squares).\n",
    "\n",
    "**Loss Function**: Least Squares/Squared Residual \n",
    "\n",
    "**Cost Function**: $l(\\hat{y_i}) = \\frac{1}{2}(y_i-\\hat{y_i})^2$\n",
    "\n",
    "**Objective Function**:\n",
    "\n",
    "$min$ $J(\\hat{\\beta_i}s) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2$ with respect to $\\hat{\\beta_i}$,\n",
    "\n",
    "where $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + ... + \\hat{\\beta_p}x_p$ \n",
    "\n",
    "**Optimization**:\n",
    "\n",
    "The estimators can be derived by solving the closed-form solutions to the optimization problem by differentiating with respect to $\\hat{\\beta_i}$. Below the steps presented in matrix forms.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "RSS(\\beta) = (y-X\\beta)^T(y-X\\beta) \\\\\n",
    "\\frac{\\partial RSS}{\\partial \\beta} = -2X^T(y-X\\beta) \\\\\n",
    "\\frac{\\partial^2 RSS}{\\partial \\beta \\partial \\beta^T} = -2X^TX \\\\\n",
    "X^T(y-X\\beta) = 0 \\\\\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In addition, the estimators can be derived using numerical methods such as Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Maximum Likelihood Estimation\n",
    "\n",
    "With the assumption that $\\epsilon$ ~ $N(0,\\sigma^2)$, the condition distribution of $y_i|x_i,\\beta_0,...,\\beta_p,\\sigma^2$ is $N(\\beta_0+\\beta_1x_1+...+\\beta_px_p,\\sigma^2)$\n",
    "\n",
    "Since the conditional pdf of $y_i$ is given by \n",
    "\n",
    "$p(y_i|x_i,\\beta_0,...,\\beta_p,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-(\\beta_0 + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + ... + \\hat{\\beta_p}x_p))^2}{2\\sigma^2}}$,\n",
    "\n",
    "the **likelehood function** is\n",
    "\n",
    "$L(\\hat{\\beta_i}s,\\sigma) = \\frac{1}{\\sqrt{2\\pi s^2}}e^{-\\frac{(y_i-(\\beta_0 + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + ... + \\hat{\\beta_p}x_p))^2}{2 s^2}}$.\n",
    "\n",
    "To derivae the estimators, proceed with with usual negative log-likelihood solution approach. The solutions will be identical to the ones from OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Regularizations\n",
    "\n",
    "Regularizations of shrinkage methods by adding an penalty term to the optimization process to constraint/shrink the the size of certain parameters, often used to control the complexity of a model to avoid overfitting.\n",
    "\n",
    "**L2/Ridge Regularization**\n",
    "\n",
    "**Penalty Term**: $\\sum_{i=1}^{n}\\beta_i^2$\n",
    "\n",
    "By adding the penalty term to the objectie function, we get\n",
    "\n",
    "$min$ $J(\\hat{y}) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-(\\hat{\\beta_0}+ \\sum_{i=1}^{n}\\beta_ix_i))^2 + \\lambda\\sum_{i=1}^{n}\\beta_i^2$ with respect to $\\hat{\\beta_i}$\n",
    "\n",
    "This has the effect of shrinking the estimates of $\\beta_i$’s. The tuning parameter $\\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates. Increasing $\\lambda$  increases bias and decreases variance (Figure 1). \n",
    "\n",
    "<img src=\"./img/3.linear_regression/figure1.png\" alt=\"Figure 1\" width=\"300\"/>\n",
    "\n",
    "With L2 regularization, the coefficients are forced to be smaller. Thus, it still includes all p features and does not apply feature selection (Figure 2).\n",
    "\n",
    "<img src=\"./img/3.linear_regression/figure2.png\" alt=\"Figure 2\" width=\"300\"/>\n",
    "\n",
    "\n",
    "**L1/Lasso Regularization**\n",
    "\n",
    "**Penalty Term**: $\\sum_{i=1}^{n}|{\\beta_i}|$\n",
    "\n",
    "By adding the penalty term to the objectie function, we get\n",
    "\n",
    "$min$ $J(\\hat{y}) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-(\\hat{\\beta_0}+ \\sum_{i=1}^{n}\\beta_ix_i))^2 + \\lambda\\sum_{i=1}^{n}|\\beta_i|$ with respect to $\\hat{\\beta_i}$\n",
    "\n",
    "As with ridge regression, the lasso shrinks the coefficient estimates towards zero with the shrinkage penalty term. However, in the case of the lasso, the L1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large (Figure 3). Hence, it performs feature selection and it yields sparse models -- that is, models that involve only a subset of the variables.\n",
    "\n",
    "<img src=\"./img/3.linear_regression/figure3.png\" alt=\"Figure 3\" width=\"300\"/>\n",
    "\n",
    "**Comparison of L1 and L2**\n",
    "\n",
    "They are two regularization methods which serve similar goals but in different manners, and neither is strictly better than another in all settings. \n",
    "\n",
    "*Pros & Cons*\n",
    "\n",
    "- L1 can address the multicollinearity problem by constraining the coefficient norm and pinning some coefficient values to 0\n",
    "- L1 is likely to remove irrelevant features, if there are any\n",
    "- L1 does not work that well when $p>n$ because it can only select $n$ features at most\n",
    "- L1 is biased towards providing sparse solutions in general, which is bad when all features are important\n",
    "- L1 is computationally more expensive than L2\n",
    "- L2 can address the multicollinearity problem by reducing the sizes of all parameters, which result in smaller variance of the estimated parameters\n",
    "- L2 can estimate a coefficient for each feature even if $p>n$\n",
    "- L2 keeps all variables so the model may be uninterpretable when the data is high-dimensional and has a substantial amount of irrelevant features\n",
    "\n",
    "*Sparsity*\n",
    "\n",
    "L1 yields shrinks some coefficients to 0 as $\\lambda$ increases, while L2 shrinks all coefficients gradually as $\\lambda$ increases. In other words, L1 prefers sparse solutions while L2 prefers more even solutions. Below are two intuitive explainations of why.\n",
    "\n",
    "I. Feasible Regions of L1/L2's Duals\n",
    "\n",
    "The duals of the OLS minimization with L1 or L2 are:\n",
    "\n",
    "- L1: $min$ $J(\\hat{y}) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-(\\hat{\\beta_0}+ \\sum_{i=1}^{n}\\beta_ix_i))^2$ where $\\sum_{i=1}^{n}|\\beta_i| \\leq s$\n",
    "- L2: $min$ $J(\\hat{y}) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-(\\hat{\\beta_0}+ \\sum_{i=1}^{n}\\beta_ix_i))^2$ where $\\sum_{i=1}^{n}\\beta_i^2 \\leq s$\n",
    "\n",
    "The constraints limit the feaisble regions for the coefficients, which are displayed accordingly in Figure 4. The original solution to the OLS minimization problem is no longer in the feasible region, and the contours represent increasing RSS values as we move away from the original solution. We keep going until we meet the boundary of the constraint. Due to the nature of round geometric regions of L2, we less likely to hit the corners that represent sparse solutions. On the other hand, L1 gives diamond-like boundaries, making it more likely to reach the corners as we deviate from the original solution.\n",
    "\n",
    "<img src=\"./img/3.linear_regression/figure4.png\" alt=\"Figure 4\" width=\"300\"/>\n",
    "\n",
    "II. L2's Nature Punish Large Parameters Over Small Ones\n",
    "\n",
    "L2 penalty term scales with large parameters since it is composed of squared sums, so it prioritizes penalizing the largest parameters. As a result, all parameters get shrinked evenly to ensure no parameter is left behind being relatively large in comparison to others. For example, suppose we have two parameters and we want to pick from $w_1=[10,0]$ and $w_2 = [5,5]$, both give the same penalty for L1, but $w_2$ is favored by L2 since it gives less penalty for L2 .\n",
    "\n",
    "\n",
    "\n",
    "**Combination of L1 and L2/Elastic Net Regularization**\n",
    "\n",
    "While combining both penalty terms, we get Elastic Net Regularization.\n",
    "\n",
    "**Penalty Term**: $\\sum_{i=1}^{n}\\beta_i^2$ and $\\sum_{i=1}^{n}|{\\beta_i}|$\n",
    "\n",
    "By adding the penalty term to the objectie function, we get\n",
    "\n",
    "$min$ $J(\\hat{y}) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i-(\\hat{\\beta_0}+ \\sum_{i=1}^{n}\\beta_ix_i))^2 + \\lambda_1\\sum_{i=1}^{n}\\beta_i^2 + \\lambda_2\\sum_{i=1}^{n}|\\beta_i|$ with respect to $\\hat{\\beta_i}$\n",
    "\n",
    "As a linear combination of L1 and L2 regularization, it produces a regularizer that has both the benefits of the L1 and L2. It may be more computationally expensive but we can potentially achieve the \"best\" combination of both by tuning the $\\lambda$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Feature Transformations\n",
    "\n",
    "When a relationship is nonlinear, it is often possible to find a transformation of the variables so\n",
    "that the relationship between the transformed variables is linear. The Circle of Transformations (Figure 5) provides a framework for helping to select a transformation for nonlinearity. This technique can also been seen as utilizing linear regression to model nonlinear relationships.\n",
    "\n",
    "Note: While power transformations may not be sensible for features with negative and positive numerical values, adding a constant ‘start’ could solve the problem.\n",
    "\n",
    "<img src=\"./img/3.linear_regression/figure5.png\" alt=\"Figure 5\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Interaction Terms\n",
    "\n",
    "Some features may have synergy effect, aka interaction effect in statistics. This is often prevalent in marketing data, where spending in one sector (TV ads) could amplify the effet of spending in another sector (e.g. Online ads). In this case, the linear combination of the features cannot capture the true relationship (data does not meet linearity assumption) and feature transformations are necessarily sensible. Instead, we can use the interaction term technique.\n",
    "\n",
    "For example, let $y$ be the number of sales, $x_1$ be the TV ads spending, and $x_2$ be the online ads spending. \n",
    "\n",
    "Using the following model\n",
    "\n",
    "$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + \\hat{\\beta_3}(x_1x_2)$ =\n",
    "\n",
    "$\\hat{y} = \\hat{\\beta_0} + (\\hat{\\beta_1} + \\hat{\\beta_3}x_2)x_1 + \\hat{\\beta_2}x_2$ or $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + (\\hat{\\beta_2} + \\hat{\\beta_3}x_1)x_2$,\n",
    "\n",
    "$\\beta_3$ captures the ampliyfing/diminish interaction effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Model Interpretations\n",
    "\n",
    "**Coefficient Surface-Level Interpretation**\n",
    "\n",
    "“A unit change in $x_j$ is associated with a $\\beta_j$ change in $Y$, while all the other variables stay fixed.”\n",
    "\n",
    "**Coefficient Statistical Significance and Confidence Interval**\n",
    "\n",
    "The standard error of an estimator reflects how it varies under repeated sampling. These standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. Standard errors can also be used to perform hypothesis tests on the coefficients. A test can be constructed to determine if there is a relationship between the feature $X$ and the outcome $Y$.\n",
    "\n",
    "In linear regression, for a parameter $\\beta_i$,\n",
    "\n",
    "*95% Confidence Interval*: \n",
    "\n",
    "$[\\beta_i - 1.96SE(\\beta_i),\\beta_i + 1.96SE(\\beta_i)]$\n",
    "\n",
    "*Test of Significance*:\n",
    "\n",
    "$H_0:\\beta_i=0$ $H_1:\\beta_i\\neq0$\n",
    "\n",
    "$t=\\frac{\\beta_i-0}{SE(\\beta_i)}$ ~ $t(n-2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample = 100\n",
    "x = np.linspace(0, 10, 100)\n",
    "X = np.column_stack((x, x**2))\n",
    "beta = np.array([1, 0.1, 10])\n",
    "e = np.random.normal(size=nsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 4.359e+06\n",
      "Date:                Fri, 04 Dec 2020   Prob (F-statistic):          5.58e-241\n",
      "Time:                        00:12:44   Log-Likelihood:                -142.51\n",
      "No. Observations:                 100   AIC:                             291.0\n",
      "Df Residuals:                      97   BIC:                             298.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.6201      0.300      2.064      0.042       0.024       1.216\n",
      "x1             0.2491      0.139      1.794      0.076      -0.026       0.525\n",
      "x2             9.9871      0.013    743.288      0.000       9.960      10.014\n",
      "==============================================================================\n",
      "Omnibus:                        0.484   Durbin-Watson:                   1.775\n",
      "Prob(Omnibus):                  0.785   Jarque-Bera (JB):                0.185\n",
      "Skew:                           0.083   Prob(JB):                        0.912\n",
      "Kurtosis:                       3.131   Cond. No.                         144.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  [0.62009831 0.24913647 9.98713752]\n",
      "R2:  0.9999888744379757\n"
     ]
    }
   ],
   "source": [
    "print('Parameters: ', results.params)\n",
    "print('R2: ', results.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5]\n",
      "2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34545455 0.34545455]\n",
      "0.1363636363636364\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0. ]\n",
      "0.20000000000000004\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Lasso(alpha=.1)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
