{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees can be used for both regression and classification.\n",
    "\n",
    "The big idea of trees is segmenting the feature space into a number of sub-regions. By growing the tree top-down in a greedy fashion, we are splitting to minimize the loss at each iteration, rather than looking ahead and minimize the overall loss. Therefore, this greedy approach does not always lead to the best/most efficient solution.\n",
    "\n",
    "For example, for regression, we can split to minimize RSS, and set stop criteria such as no regions contain more than 5 data points. We can construct the function/mapping based on the average from each region.\n",
    "\n",
    "Combining multiple trees (i.e. bagging, random forests, boosting) can often result in significant improvements in prediction performance at the cost of some loss in the model's interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are prune of overfitting as they can always achieve perfect training accuracy if grown infinitely. Pruning a tree is an effective way to avoid overfitting.\n",
    "\n",
    "We can\n",
    "- Pre-specify the depth, or\n",
    "- Stop once the reduction is loss is less than some threshold\n",
    "\n",
    "but both strategies are not optimal since they are blind or short-sighted. Another good strategy is to grow a large tree then prune it to achieve a optimal subtree, and this is called **cost complexity pruning**. This is done through a similar manner to applying regularizations, where we can add $\\alpha|T|$ to the cost function, where $|T|$ indicates the number of terminal nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to optimizing around RSS for regression trees, we optimize around **Gini index** or **Cross-entropy** for classification trees. They are similar numerically and both are measures of uncertainty or variance. The tree will be grown in a greedy fashion to minimize them, or to maximize gini gain (reduction in Gini index) or information gain (reduction in Cross-entropy) during each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**\n",
    "- Trees are easy to explain to people, even easier than linear regression or logistic regression as it is more similar to human decision-making\n",
    "- Easy to visualize and can be interpreted by non-technical people (if short enough)\n",
    "- Scale well with n as growing a tree is a divide-and-conquer operation\n",
    "- Handle mixed features (numerical/categorical) well\n",
    "\n",
    "**Cons**\n",
    "- Usually sub-optimal predictive power\n",
    "- Vulnerable to overfitting and high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
