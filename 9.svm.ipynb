{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "A more flexible or more \"powerful\" version of perceptron, both try to form a hyperplane to separate the classes in feature space. Not really a probabilistic approach, more of a computational optimization method.\n",
    "\n",
    "While the data is not linearly separable, SVM can\n",
    "- soften the idea of \"separating\"\n",
    "- enrich or enlarge the feature space so that the separation becomes possible\n",
    "\n",
    "A **hyperplance** in $p$ dimensions is a flat affine subspace of dimension $p-1$. In otherwords, form a function, $f(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p$, such that $f(X)>0$ for data points in one class and $f(X)<0$ for data points in another class.\n",
    "\n",
    "It is a maximal margin classifier, finding the hyperplane that makes the the biggest gap or magin between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work on non-linearly separable data, allow some mistakes/slack meansured in relative to the size of the margin, controlled by some budget, C, a tuning parameter to control the sub-margins. Tuning C is playing around with the bias-variance trade-off.\n",
    "\n",
    "Sometimes linear separator simply won't fit to the data despite high C. We can enlarge the space of features by transformations, going from p dimensions to p+ dimensions (e.g. x to (x,x^2)). This results in non-linear decision boundaries in the original space.\n",
    "\n",
    "One way to do this is through **kernels**. If we can compute inner-products between observations, we can fit a SV classifier, and some special kernal functions can do this for us.\n",
    "\n",
    "Some common kernels are:\n",
    "- Polynomial kernel: popular in image processing\n",
    "- Gaussian kernel: general-purpose kernel, used when there is no prior knowledge about the data\n",
    "- RBF kernel: general-purpose kernel, used when there is no prior knowledge about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation\n",
    "\n",
    "**Loss Function**: Hinge-loss\n",
    "\n",
    "<img src=\"./img/9.svm/figure1.png\" alt=\"Figure 1\" width=\"300\"/>\n",
    "\n",
    "<img src=\"./img/9.svm/figure2.png\" alt=\"Figure 2\" width=\"500\"/>\n",
    "\n",
    "**Cost Function**: $l(\\hat{y_i}) =$ \n",
    "- $max(0,1-\\beta^TX)$ if $y=1$\n",
    "- $max(0,1+\\beta^TX)$ if $y=0$\n",
    "\n",
    "**Objective Function**:\n",
    "\n",
    "$min$ $J({\\beta}) = \\sum_{i=1}^{n} y*max(0,1-\\beta^TX) + (1-y)*max(0,1+\\beta^TX))$ with respect to $\\hat{\\beta_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM vs. Logistic Regression**\n",
    "\n",
    "- When classes are (nearly) separable, SVM generally does better than LR\n",
    "- When not, LR(with ridge penalty) and SVM are similar\n",
    "- LR is probablistic and SVM is not\n",
    "- Kernel SVM is good for forming nonlinear boundaries (while we can use kernels with LR as well, the computations are more expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
