{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN or MLP\n",
    "\n",
    "**Structure**\n",
    "- Input nodes\n",
    "- Hidden layers\n",
    "- Output nodes\n",
    "\n",
    "**Details**:\n",
    "- A set of weights representing the connections between each neural network layer and the next layer\n",
    "- A set of biases, one for each node.\n",
    "- An activation function that transforms the output of each node in a layer. Different layers may have different activation functions. (Common choices: softmax, relu, sigmoid)\n",
    "\n",
    "Activation function is used to introduce non-linearlity to the model, which gives the model the complexity to fit to non-linearly separable data. The weights and the biases are the estimators, and they can be estimated by backpropagation. \n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "It estimates the weights and biases backwards using chain rule and gradient descent. We can express the input to the output node as a function of the previous weights,biaes, and activiation function and subsitute that expression into our choice of loss function, we can then compute the gradient of the expression and perform numerical optimization method such as gradient descent to update the weights and biases accordingly to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "Suitable for \n",
    "- Time Series data\n",
    "- Text data\n",
    "- Audio data\n",
    "\n",
    "Structure\n",
    "- Input nodes\n",
    "- Hidden layers (w/ looping constraint )\n",
    "- Output nodes\n",
    "\n",
    "\"A looping constraint on the hidden layer of ANN turns to RNN.\"\n",
    "\n",
    "RNN has a recurrent connection on the hidden state. This looping constraint ensures that sequential information is captured in the input data. RNN captures the sequential information present in the input data (i.e. dependency between the words in the text while making predictions.)\n",
    "\n",
    "RNNs share the parameters across different time steps. This is popularly known as Parameter Sharing. This results in fewer parameters to train and decreases the computational cost\n",
    "\n",
    "Note: Deep RNNs (RNNs with a large number of time steps) also suffer from the vanishing and exploding gradient problem which is a common problem in all the different types of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "Suitable for\n",
    "- Image data\n",
    "\n",
    "The building blocks of CNNs are filters a.k.a. kernels. Kernels are used to extract the relevant features from the input using the convolution operation.\n",
    "\n",
    "In CNNs, the layers are threedimensional. This means that the neurons are structured in shape of form (width, height, depth)\n",
    "\n",
    "Structure\n",
    "- Input nodes\n",
    "- Convolutional layers: serves to detect (multiple) patterns in multipe sub-regions in the input field using receptive fields\n",
    "- Activation function (Relu) in conjunction with the convolutional layer\n",
    "- Pooling layers: reduce the spatial size of the representation, to reduce the number of parameters and amount of computation in the network, and hence to also control overfitting\n",
    "- Fullly connected layer: A final layer that is connected to all activiations from the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Hyperparameters\n",
    "\n",
    "Optimization Hyperparameters:\n",
    "- Learning rate\n",
    "- Mini-Batch Size\n",
    "- Number of Epochs\n",
    "\n",
    "Model Hyperparameters:\n",
    "- Number of hidden layers\n",
    "- Number of hidden units\n",
    "- Dropout rate\n",
    "- Activiation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Overfitting\n",
    "\n",
    "- Add (higher) dropout\n",
    "- Adding regularization terms (e.g L1,L2) to the loss function\n",
    "- Early stopping (epochs)\n",
    "- Data augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
