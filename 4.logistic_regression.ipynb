{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Intro\n",
    "\n",
    "Classification is the task of choosing a value of $y$ that maximizes $P(Y|X)$. Naive Bayes worked by approximating that probability using the naive assumption that each feature was independent given the class label.\n",
    "\n",
    "**Model**\n",
    "\n",
    "We assume a model $P(Y=1|X) = S(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)$, where y is a binary variable (i.e. 1 or 0) and S is the sigmoid function, $S(x)=\\frac{1}{1+e^{-x}}$, \n",
    "\n",
    "and fit \n",
    "\n",
    "$\\hat{P(y=1|X)} = 1/(1+e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)}) = \n",
    "\\frac{e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p}}{1+e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p}}$\n",
    "\n",
    "to the data by estimating $\\hat{\\beta_i}$'s.\n",
    "\n",
    "The sigmoid function maps the \"output from the linear regression model\" to be between $[0,1]$, representing the probablity of $Y=1$ given $X$.\n",
    "\n",
    "**Model Assumptions**:\n",
    "- $Y$ ~ *Bernoulli*($S(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)$), that is,  \n",
    "$P(Y=1|X) = S(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)$ and $P(Y=0|X) = 1-S(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p)$\n",
    "- Each feature was independent given the class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Log-loss/Cross-entropy\n",
    "\n",
    "**Loss Function**: Log-loss/Cross-entropy \n",
    "\n",
    "<img src=\"./img/4.logistic_regression/figure1.png\" alt=\"Figure 1\" width=\"450\"/>\n",
    "\n",
    "**Cost Function**:\n",
    "\n",
    "$l(\\hat{P(y_i=1|X_i)}) = -ylog(S(\\beta^TX)) - (1-y)log(1-S(\\beta^TX))$\n",
    "- $-log(\\hat{P(y=1|X)}) = -log(S(\\beta^TX))$ when $y=1$\n",
    "- $-log(1-\\hat{P(y=1|X)}) = -log(1-S(\\beta^TX))$ when $y=0$\n",
    "\n",
    "**Objective Function**:\n",
    "\n",
    "$min$ $J({\\beta}) = \\frac{1}{n}\\sum_{i=1}^{n} -ylog(S(\\beta^TX)) - (1-y)log(1-S(\\beta^TX))$ with respect to $\\hat{\\beta_j}$, where $S(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "**Optimization**:\n",
    "\n",
    "There is not closed-form solution to this problem. We can apply **Gradient Descent** to find the parameters which potentially minimize the objective function. Gradient Descent is a optimization algorithm. It takes partial derivative of $J$ with respect to $\\beta$ (the slope of $J$), and updates $\\beta$ via each iteration with a selected learning rate $\\alpha$ until the Gradient Descent has converged.\n",
    "\n",
    "**Gradient Descent**\n",
    "\n",
    "In this algorithm, we calculate the gradient of cost function in every iteration and update the values of parameters simultaneously using the formula\n",
    "\n",
    "$\\beta_i^{new} = \\beta_i^{old} - \\alpha \\frac{\\partial}{\\partial \\beta_i} J({\\beta})$\n",
    "\n",
    "Normal gradient descent uses the complete dataset available to compute the gradient of cost function. As we need to calculate the gradient on the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that don’t fit in memory.\n",
    "\n",
    "*Two variations of Gradient Descent*\n",
    "- Stochastic Gradient Descent: Use only one training example in every iteration to compute the gradient of cost function. \n",
    "- Mini-Batch Gradient Descent: Use a set of ‘m’ training examples called batch in every iteration to compute the gradient of cost function.\n",
    "\n",
    "<img src=\"./img/4.logistic_regression/figure2.png\" alt=\"Figure 2\" width=\"300\"/>\n",
    "\n",
    "Stochastic Gradient uses one training example in every iteration this algo is faster for larger data set. In SGD, one might not achieve accuracy, but the computation of results are faster. Mini batch algorithm is the most favorable and widely used algorithm that makes precise and faster results using a batch of training examples. Common mini-batch sizes range between 50 and 256, but can vary for different applications.\n",
    "\n",
    "*The Choice of $\\alpha$/Learning Rate*\n",
    "\n",
    "- If $\\alpha$ is too large, the algorithm would take larger steps and algorithm may not converge\n",
    "- If $\\alpha$ is too small, the algorithm will take too many iterations and too long to converge\n",
    "\n",
    "<img src=\"./img/4.logistic_regression/figure3.png\" alt=\"Figure 3\" width=\"450\"/>\n",
    "\n",
    "Plotting the curve between Number of Iterations and value of cost function helps to identify whether gradient descent is working properly or not.\n",
    "\n",
    "<img src=\"./img/4.logistic_regression/figure4.jpeg\" alt=\"Figure 4\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Maximum Likelihood Estimation\n",
    "\n",
    "https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizations\n",
    "\n",
    "Same as linear regression, adding the penalty terms to the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "$log(\\frac{P}{1-P}) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p$, where $P=P(Y=1)$.\n",
    "\n",
    "“A unit change in $x_j$ is associated with a $\\beta_j$ change in the log odds or a $e^{\\beta}$ change in the odds, while all the other variables stay fixed.”\n",
    "\n",
    "The odds of Y=1 are defined as the ratio of the probability of $Y=1$ over the probability of $Y=0$.  For example, if the odds of $Y=1$ are .8/.2 = 4.  That is to say that the odds of $Y=1$ are  4 to 1.  If the probability of $Y=1$ is .5, i.e., 50-50 percent chance, then the odds of $Y=1$ is 1 to 1.\n",
    "\n",
    "https://nhorton.people.amherst.edu/ips9/IPS_09_Ch14.pdf\n",
    "\n",
    "The test statistics of the coefficients follow standard normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
